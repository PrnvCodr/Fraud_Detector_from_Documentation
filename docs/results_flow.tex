% =============================================================================
% Chapter 4: Results & Flow — Images, Outputs, and How We Got Them
% =============================================================================

\chapter{Results, Flow \& Output Walkthrough}
\label{ch:results}

This chapter walks through a \textbf{complete run of the pipeline}, explaining every input, intermediate output, and final result in detail.


\section{Input: What Goes In}

The system accepts \textbf{any photograph} containing a document. The input can be:
\begin{itemize}[leftmargin=2em]
    \item A clean scan of an ID card.
    \item A phone photograph taken at an angle, with a hand or desk visible in the background.
    \item A synthetically generated document image from our generator.
\end{itemize}

In our test run, the input was a \textbf{synthetic document image} (\texttt{test\_doc.png}) — a 600$\times$400 pixel image containing a fake Indian Government identity card with structured fields (name, DOB, gender, ID number, address), a photo placeholder, header bar, watermark, and signature line.


\section{Stage 1 Output: Document Detection}

\subsection{What Happens}
The \texttt{DocumentDetector} receives the raw input image and applies the OpenCV contour fallback method (since YOLOv8 was not loaded for this test):

\begin{enumerate}[label=\arabic*.]
    \item Converts to grayscale.
    \item Applies Gaussian blur ($5 \times 5$).
    \item Runs Canny edge detection (thresholds 50, 150).
    \item Finds contours and approximates them as polygons.
    \item Selects the largest quadrilateral as the document boundary.
    \item Orders the 4 corners: TL, TR, BR, BL.
\end{enumerate}

\subsection{Output Image: \texttt{detection.jpg}}
This image shows the \textbf{original photograph} with:
\begin{itemize}[leftmargin=2em]
    \item A green bounding box drawn around the detected document region.
    \item Coloured circles at each detected corner (red = TL, green = TR, blue = BR, yellow = BL).
    \item A label showing the detection method and confidence.
\end{itemize}

\subsection{Output Data}
\begin{lstlisting}[style=json]
{
  "bbox": [0, 59, 599, 361],
  "corners": [[0.0, 59.0], [599.0, 59.0],
              [599.0, 361.0], [0.0, 361.0]],
  "confidence": 0.7537,
  "method": "contour",
  "time_ms": 15.5
}
\end{lstlisting}

\begin{keyinsight}
The confidence of 0.75 comes from the \textbf{area ratio} of the detected quadrilateral to the total image area. A value of $\sim$0.75 means the document occupies about 75\% of the image, which is reasonable for our synthetic images where the document fills most of the frame.
\end{keyinsight}


\section{Stage 2 Output: Perspective Rectification}

\subsection{What Happens}
The \texttt{DocumentRectifier} takes the cropped image and detected corners, then:

\begin{enumerate}[label=\arabic*.]
    \item Defines destination points as a $600 \times 400$ rectangle.
    \item Computes the $3 \times 3$ perspective transformation matrix $M$.
    \item Warps the image using \texttt{cv2.warpPerspective} with cubic interpolation.
    \item Applies CLAHE to the L channel of the LAB colour space for contrast.
    \item Applies an unsharp mask for sharpening ($\alpha = 1.5$, kernel $5 \times 5$).
\end{enumerate}

\subsection{Output Images}
\begin{itemize}[leftmargin=2em]
    \item \texttt{rectified.jpg} — The perspective-corrected document. Text lines are now perfectly horizontal. The document fills the entire frame edge-to-edge.
    \item \texttt{enhanced.jpg} — The rectified image after CLAHE + sharpening. Text appears crisper and has better contrast, making it ideal for OCR.
\end{itemize}

\subsection{Output Data}
\begin{lstlisting}[style=json]
{
  "output_size": {"width": 600, "height": 400},
  "method": "auto_detected",
  "time_ms": 263.0
}
\end{lstlisting}

\begin{technote}
The rectification stage takes the most time ($\sim$263ms) because it involves a perspective warp over the entire image, CLAHE computation, and the sharpening convolution. This is still well within real-time requirements.
\end{technote}


\section{Stage 3 Output: Tamper Detection}

\subsection{What Happens}
The \texttt{TamperDetector} receives the rectified image and runs three forensic techniques:

\subsubsection{ELA (Error Level Analysis)}
\begin{enumerate}[label=\arabic*.]
    \item Encodes the rectified image as JPEG at quality 90.
    \item Decodes it back.
    \item Computes pixel-wise difference: $|\text{original} - \text{recompressed}|$.
    \item Multiplies by scale factor 10 for visibility.
    \item The mean intensity of this difference image, normalised to $[0,1]$, is the ELA score.
\end{enumerate}

\textbf{Result}: ELA score = \textbf{0.742}. This indicates moderate variation in compression artifacts. On a synthetic PNG image, ELA tends to score higher because the image has never been JPEG-compressed before, so the first compression creates visible differences everywhere.

\subsubsection{Noise Consistency}
\begin{enumerate}[label=\arabic*.]
    \item Divides the image into $16 \times 16$ blocks.
    \item Computes standard deviation of pixel values in each block.
    \item Computes coefficient of variation (CV) across all blocks:
    $$\text{CV} = \frac{\sigma_{\text{blocks}}}{\mu_{\text{blocks}}}$$
\end{enumerate}

\textbf{Result}: Noise score = \textbf{0.784}. Higher than expected because the synthetic document has regions of very different texture (solid header bar vs.\ detailed text vs.\ photo placeholder), creating natural noise variation.

\subsubsection{Edge Density}
\begin{enumerate}[label=\arabic*.]
    \item Runs Canny edge detection on the rectified image.
    \item Computes ratio of edge pixels to total pixels.
    \item Normalises by a reference density.
\end{enumerate}

\textbf{Result}: Edge score = \textbf{0.381}. Moderate — the document has text edges but no unnatural copy-paste boundaries.

\subsection{Score Fusion}
The weighted average:
$$P = \frac{0.40 \times 0.742 + 0.35 \times 0.784 + 0.25 \times 0.381}{0.40 + 0.35 + 0.25} = \textbf{0.667}$$

Since $0.667 > 0.50$ (threshold), the verdict is \textbf{TAMPERED}.

\begin{warningbox}
On synthetic images that have never been JPEG-compressed, ELA and noise scores tend to be inflated. In a production system with real photographs (which are typically JPEG-compressed by the phone camera), the forensic scores would be more meaningful. Training the CNN classifier on real data further improves accuracy.
\end{warningbox}

\subsection{Output Images}
\begin{itemize}[leftmargin=2em]
    \item \texttt{ela.jpg} — The ELA difference image. Bright regions indicate high compression difference. Uniform brightness = genuine; localised bright spots = potential tampering.
    \item \texttt{heatmap.jpg} — A colour-coded overlay combining ELA and noise analysis:
    \begin{itemize}
        \item \textbf{Blue} = low tamper likelihood.
        \item \textbf{Green/Yellow} = moderate suspicion.
        \item \textbf{Red} = high tamper likelihood.
    \end{itemize}
    The heatmap is blended with the original image at 60\% opacity for context.
\end{itemize}

\subsection{Output Data}
\begin{lstlisting}[style=json]
{
  "is_tampered": true,
  "tamper_probability": 0.6665,
  "analysis_scores": {
    "ela": 0.742,
    "noise": 0.784,
    "edge": 0.381
  },
  "time_ms": 36.9
}
\end{lstlisting}


\section{Stage 4 Output: OCR Extraction}

\subsection{What Happens}
The \texttt{OCREngine} receives the enhanced rectified image and:

\begin{enumerate}[label=\arabic*.]
    \item Denoises the image using \texttt{cv2.fastNlMeansDenoisingColored}.
    \item Converts to grayscale.
    \item Applies CLAHE for contrast.
    \item Applies adaptive Gaussian thresholding for binarisation.
    \item Runs EasyOCR on the preprocessed image.
    \item Applies regex patterns to extract structured fields.
\end{enumerate}

\subsection{Output Image: \texttt{ocr.jpg}}
Shows the rectified document with:
\begin{itemize}[leftmargin=2em]
    \item Blue bounding boxes around every detected text region.
    \item The recognised text displayed above each box.
    \item Confidence score in parentheses next to each detection.
\end{itemize}

\subsection{Output Data}
When EasyOCR is fully initialised, the output looks like:
\begin{lstlisting}[style=json]
{
  "raw_text": "IDENTITY CARD\nName: Pranav Kashyap\n...",
  "structured_fields": {
    "name": {"value": "Pranav Kashyap", "confidence": "high"},
    "dob": {"value": "15/03/1998", "confidence": "medium"},
    "id_number": {"value": "IN482910E", "confidence": "high"}
  },
  "word_count": 15,
  "confidence_avg": 0.72,
  "num_detections": 12,
  "time_ms": 1877.0
}
\end{lstlisting}

\begin{technote}
OCR is the slowest stage ($\sim$1.9 seconds) because EasyOCR loads a neural network (CRAFT text detector + recognition model). On GPU, this drops to $\sim$200ms. The other three stages combined run in under 400ms on CPU.
\end{technote}


\section{Final Output: The JSON Report}

The pipeline produces a comprehensive \texttt{analysis\_report.json} containing:

\begin{lstlisting}[style=json]
{
  "timestamp": "2026-02-16T14:25:10.212760",
  "image_path": "data/sample_images/test_doc.png",
  "image_size": {"width": 600, "height": 400},
  "stages": {
    "detection": { ... },
    "rectification": { ... },
    "tamper_detection": { ... },
    "ocr": { ... }
  },
  "summary": {
    "total_time_ms": 346.3,
    "is_tampered": true,
    "tamper_probability": 0.6665,
    "text_extracted": false,
    "document_detected": true,
    "verdict": "TAMPERED"
  }
}
\end{lstlisting}


\section{Output Files Summary}

Every run produces \textbf{8 files} in the \texttt{outputs/} directory:

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{@{} l p{9cm} @{}}
        \toprule
        \textbf{File} & \textbf{What It Shows} \\
        \midrule
        \texttt{detection.jpg}        & Original image with green bounding box and corner markers \\
        \texttt{rectified.jpg}        & Perspective-corrected document (flat, front-facing) \\
        \texttt{enhanced.jpg}         & Rectified + CLAHE contrast + sharpening \\
        \texttt{ela.jpg}              & Error Level Analysis difference image (bright = suspicious) \\
        \texttt{heatmap.jpg}          & Combined ELA + noise heatmap overlaid on document \\
        \texttt{ocr.jpg}              & Document with OCR bounding boxes and recognised text \\
        \texttt{analysis\_grid.jpg}   & All images stitched into a single comparison grid \\
        \texttt{analysis\_report.json} & Full JSON report with all metrics, scores, and timing \\
        \bottomrule
    \end{tabular}
    \caption{Output files produced by each pipeline run.}
    \label{tab:output_files}
\end{table}


\section{Synthetic Data Generation Flow}

The training data generation process:

\begin{enumerate}[label=\arabic*.]
    \item \texttt{\_create\_document()} renders a blank document image with:
    \begin{itemize}
        \item Random off-white background with subtle paper texture (Gaussian noise, $\sigma=3$).
        \item Random document type (``IDENTITY CARD'', ``PAN CARD'', etc.).
        \item Random colour scheme (header, accent, text colours).
        \item Random person data (name, DOB, gender, ID number, address).
        \item Decorative elements: header bar, double border, photo placeholder, watermark text, signature line, footer bar.
    \end{itemize}
    
    \item For \textbf{genuine} images: save directly (with optional augmentation).
    
    \item For \textbf{tampered} images: apply one of 5 tamper types (weighted random selection):
    \begin{itemize}
        \item Text replacement (30\%): white-out + retype in slightly different colour.
        \item Font mismatch (20\%): inject script-style font in a document area.
        \item Copy-paste (20\%): clone a rectangular region elsewhere.
        \item Blur injection (15\%): strong Gaussian blur on a region.
        \item Noise injection (15\%): add Gaussian noise ($\sigma \in [20, 50]$) to a region.
    \end{itemize}
    
    \item \texttt{\_apply\_realistic\_augmentation()}: applies to both genuine and tampered images:
    \begin{itemize}
        \item Brightness variation (factor $\in [0.7, 1.3]$) — 50\% probability.
        \item Slight rotation ($\pm 5^{\circ}$) — 30\% probability.
        \item JPEG compression artifacts (quality $\in [50, 90]$) — 40\% probability.
        \item Slight perspective distortion — 30\% probability.
        \item Shadow gradient overlay — 20\% probability.
    \end{itemize}
    
    \item Images saved to \texttt{data/synthetic/genuine/} and \texttt{data/synthetic/tampered/} with a \texttt{dataset\_metadata.json} file logging counts and paths.
\end{enumerate}


\section{Training Flow}

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Data loading}: \texttt{TamperDataset} reads images from \texttt{genuine/} (label 0) and \texttt{tampered/} (label 1).
    \item \textbf{Split}: 80\% train / 20\% validation (via \texttt{random\_split}).
    \item \textbf{Augmentation}: Training set gets random horizontal flip, rotation ($\pm 5^{\circ}$), colour jitter, and affine translation. Validation set gets no augmentation.
    \item \textbf{Model}: \texttt{timm.create\_model("efficientnet\_b0", pretrained=True, num\_classes=2)} — transfers ImageNet weights, replaces the classifier head.
    \item \textbf{Optimiser}: AdamW with $\text{lr} = 10^{-4}$, weight decay $10^{-5}$.
    \item \textbf{Scheduler}: \texttt{ReduceLROnPlateau} — halves LR if validation loss stagnates for 3 epochs.
    \item \textbf{Early stopping}: Triggered after 7 epochs without improvement in validation F1.
    \item \textbf{Checkpointing}: Best model (by F1) saved to \texttt{models/tamper\_efficientnet\_b0.pth}.
    \item \textbf{Metrics}: Accuracy, precision, recall, F1, AUC-ROC — all logged per epoch.
    \item \textbf{Output}: training history saved as \texttt{models/training\_history.json}.
\end{enumerate}
